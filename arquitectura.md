  
Esta arquitectura sigue un principio claro:

Un solo punto de verdad para modelos y claves.
Servicios especializados, sin duplicaciÃ³n de clientes.
main.py como orquestador, no como cerebro.

ğŸ§© Componentes Principales
1ï¸âƒ£ LLMSettings

ğŸ“ Ãšnico sitio donde se definen claves, modelos y clientes

Responsabilidad

Cargar variables de entorno (.env)

Definir modelos activos (chat, clasificaciÃ³n, embeddings)

Crear y mantener una Ãºnica instancia de cada cliente LLM

Gestiona

OPENAI_API_KEY

ANTHROPIC_API_KEY

LMSTUDIO_BASE_URL

Clientes creados

llm.openai_client â†’ OpenAI (cloud)

llm.anthropic_client â†’ Claude (preparado, opcional)

llm.lmstudio_client â†’ LM Studio (local)

Modelos definidos

Chat (cloud): gpt-5-nano

Chat (local): openai/gpt-oss-20b

ClasificaciÃ³n emocional: gpt-5-nano

Embeddings (FAISS): text-embedding-3-small

ğŸ”’ NingÃºn otro archivo crea clientes ni lee claves directamente.


2ï¸âƒ£ ClassifierService

ğŸ“ ClasificaciÃ³n emocional y de riesgo (cloud-only)

Responsabilidad

Analizar el mensaje actual del usuario

Devolver un JSON estructurado con:

mood

intensity

tone_hint

risk

topics

CaracterÃ­sticas

Usa exclusivamente llm.openai_client

No usa historial (solo el mensaje actual)

Respuestas cortas y baratas (â‰ˆ120 tokens)

Parseo robusto (tolerante a ruido)

MotivaciÃ³n

La clasificaciÃ³n debe ser estable y fiable

Los modelos locales no garantizan JSON estricto

3ï¸âƒ£ RagFaissService

ğŸ“ RecuperaciÃ³n de contexto (RAG) con FAISS

Responsabilidad

Cargar y trocear playbooks (.md)

Crear embeddings y un Ã­ndice FAISS

Recuperar contexto relevante para un mensaje

CaracterÃ­sticas

Usa llm.openai_client inyectado

Usa llm.embedding_model

Cachea Ã­ndices en disco

No crea clientes propios

Si no hay API key â†’ devuelve contexto vacÃ­o (graceful)

Resultado

Devuelve texto contextual que se inyecta como:
[Playbook guidance]
...


4ï¸âƒ£ ChatbotService

ğŸ“ GeneraciÃ³n de respuesta conversacional

Responsabilidad

Construir instrucciones finales del modelo

Generar la respuesta del asistente

Clientes usados

llm.openai_client â†’ si use_local = false

llm.lmstudio_client â†’ si use_local = true

QuÃ© recibe

message

history (sanitizado)

setting (fusionado con clasificaciÃ³n + RAG)

QuÃ© NO hace

No clasifica emociones

No crea clientes

No accede a .env

6ï¸âƒ£ AuthService

ğŸ“ SesiÃ³n y autenticaciÃ³n (cookie-based)

Responsabilidad

Leer la cookie de sesiÃ³n (`COOKIE_NAME`, default `oms_session`)

Serializar/deserializar sesiÃ³n con `SESSION_SECRET`

Exponer helpers:

`get_current_user(request)` â†’ `dict | None` (no fuerza auth)

`require_auth(user)` â†’ 401 si no hay usuario

Endpoint relacionado

`GET /api/me` â†’ â€œmeâ€ fuerte (requiere auth, puede responder 401)

7ï¸âƒ£ UsageService

ğŸ“ LÃ­mite de mensajes + cookie de invitado

Responsabilidad

Gestionar cookie invitado (`GUEST_COOKIE_NAME`, default `oms_guest`)

Contar turnos de usuario en `history`

Enforzar lÃ­mites (paywall 402) para:

AnÃ³nimo â†’ `ANON_MESSAGE_LIMIT` (default 10)

Autenticado â†’ `AUTH_MESSAGE_LIMIT` (default 100)

5ï¸âƒ£ main.py

ğŸ“ Orquestador limpio

Responsabilidad

Instanciar servicios una sola vez

Coordinar el flujo de /api/chat

Exponer endpoints â€œpolicyâ€ sin forzar auth

Flujo tÃ­pico

Recibe request del frontend

rag_context = rag.retrieve(message)

classification = classifier.classify(message)

Fusiona:

setting del frontend

hints de clasificaciÃ³n (sin pisar)

contexto RAG

response = chatbot.chat(...)

Devuelve:

{
  "response": "...",
  "model": "...",
  "classification": {...},
  "risk": "none"
}

Endpoints relacionados (auth + lÃ­mites)

`GET /api/me/policy` â†’ siempre 200, devuelve:

`isAuthenticated` + `maxMessages` + `limits{anon/auth}` (+ `user` opcional si logged)

`GET /api/me` â†’ requiere auth (401 si no logged)

Ventajas

FÃ¡cil de leer

FÃ¡cil de testear

FÃ¡cil de extender (auth, DB, mÃ©tricas)

ğŸ—ï¸ Esquema Visual (mental)


             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚ LLMSettings â”‚
             â”‚ (keys+LLMs) â”‚
             â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚              â”‚              â”‚
â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Classifierâ”‚  â”‚ RagFaiss   â”‚  â”‚ Chatbot      â”‚
â”‚Service   â”‚  â”‚ Service    â”‚  â”‚ Service      â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚             â”‚               â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
              â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
              â”‚ main.py  â”‚
              â”‚ Orquest. â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¯ Beneficios de esta Arquitectura

âœ… Cero duplicaciÃ³n de clientes

âœ… Cero errores por .env en import-time

âœ… Cloud y local conviven sin fricciÃ³n

âœ… FÃ¡cil aÃ±adir:

Claude

Fine-tuning

AutenticaciÃ³n

Persistencia

âœ… Ideal para MVP â†’ producciÃ³n       
